{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c66d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "demand_hist = []\n",
    "for i in range(52):\n",
    "    for j in range(4):\n",
    "        random_demand = np.random.normal(3, 1.5)\n",
    "        if random_demand < 0:\n",
    "            random_demand = 0\n",
    "        random_demand = np.round(random_demand)\n",
    "        demand_hist.append(random_demand)\n",
    "    random_demand = np.random.normal(6, 1)\n",
    "    if random_demand < 0:\n",
    "        random_demand = 0\n",
    "    random_demand = np.round(random_demand)\n",
    "    demand_hist.append(random_demand)\n",
    "    for j in range(2):\n",
    "        random_demand = np.random.normal(12, 2)\n",
    "        if random_demand < 0:\n",
    "            random_demand = 0\n",
    "        random_demand = np.round(random_demand)\n",
    "        demand_hist.append(random_demand)\n",
    "plt.hist(demand_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb052e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvOptEnv():\n",
    "    def __init__(self, demand_records):\n",
    "        self.n_period = len(demand_records)\n",
    "        self.current_period = 1\n",
    "        self.day_of_week = 0\n",
    "        self.inv_level = 25\n",
    "        self.inv_pos = 25\n",
    "        self.capacity = 50\n",
    "        self.holding_cost = 3\n",
    "        self.unit_price = 30\n",
    "        self.fixed_order_cost = 50\n",
    "        self.variable_order_cost = 10\n",
    "        self.lead_time = 2\n",
    "        self.order_arrival_list = []\n",
    "        self.demand_list = demand_records\n",
    "        self.state = np.array([self.inv_pos] + self.convert_day_of_week(self.day_of_week))\n",
    "        self.state_list = []\n",
    "        self.state_list.append(self.state)\n",
    "        self.action_list = []\n",
    "        self.reward_list = []\n",
    "            \n",
    "    def reset(self):\n",
    "        self.state_list = []\n",
    "        self.action_list = []\n",
    "        self.reward_list = []\n",
    "        self.inv_level = 25\n",
    "        self.inv_pos = 25\n",
    "        self.current_period = 1\n",
    "        self.day_of_week = 0\n",
    "        self.state = np.array([self.inv_pos] + self.convert_day_of_week(self.day_of_week))\n",
    "        self.state_list.append(self.state)\n",
    "        self.order_arrival_list = []\n",
    "        return self.state\n",
    "        \n",
    "    def step(self, action):\n",
    "        if action > 0:\n",
    "            y = 1\n",
    "            self.order_arrival_list.append([self.current_period+self.lead_time, action])\n",
    "        else:\n",
    "            y = 0\n",
    "        if len(self.order_arrival_list) > 0:\n",
    "            if self.current_period == self.order_arrival_list[0][0]:\n",
    "                self.inv_level = min(self.capacity, self.inv_level + self.order_arrival_list[0][1])\n",
    "                self.order_arrival_list.pop(0)  \n",
    "        demand = self.demand_list[self.current_period-1]\n",
    "        units_sold = demand if demand <= self.inv_level else self.inv_level\n",
    "        reward = units_sold*self.unit_price-self.holding_cost*self.inv_level - y*self.fixed_order_cost \\\n",
    "                 -action*self.variable_order_cost    \n",
    "        self.inv_level = max(0,self.inv_level-demand)\n",
    "        self.inv_pos = self.inv_level\n",
    "        if len(self.order_arrival_list) > 0:\n",
    "            for i in range(len(self.order_arrival_list)):\n",
    "                self.inv_pos += self.order_arrival_list[i][1]\n",
    "        self.day_of_week = (self.day_of_week+1)%7\n",
    "        self.state = np.array([self.inv_pos] +self.convert_day_of_week(self.day_of_week))\n",
    "        self.current_period += 1\n",
    "        self.state_list.append(self.state)\n",
    "        self.action_list.append(action)\n",
    "        self.reward_list.append(reward)\n",
    "        if self.current_period > self.n_period:\n",
    "            terminate = True\n",
    "        else: \n",
    "            terminate = False\n",
    "        return self.state, reward, terminate\n",
    "    \n",
    "    def convert_day_of_week(self,d):\n",
    "        if d == 0:\n",
    "            return [0, 0, 0, 0, 0, 0]\n",
    "        if d == 1:\n",
    "            return [1, 0, 0, 0, 0, 0] \n",
    "        if d == 2:\n",
    "            return [0, 1, 0, 0, 0, 0] \n",
    "        if d == 3:\n",
    "            return [0, 0, 1, 0, 0, 0] \n",
    "        if d == 4:\n",
    "            return [0, 0, 0, 1, 0, 0] \n",
    "        if d == 5:\n",
    "            return [0, 0, 0, 0, 1, 0] \n",
    "        if d == 6:\n",
    "            return [0, 0, 0, 0, 0, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7621781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\" Actor (Policy) Model.\"\"\"\n",
    "    def __init__(self, state_size, action_size, seed, fc1_unit=128,\n",
    "                 fc2_unit = 128):\n",
    "        \"\"\"\n",
    "        Initialize parameters and build model.\n",
    "        Params\n",
    "        =======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_unit (int): Number of nodes in first hidden layer\n",
    "            fc2_unit (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork,self).__init__() ## calls __init__ method of nn.Module class\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1= nn.Linear(state_size,fc1_unit)\n",
    "        self.fc2 = nn.Linear(fc1_unit,fc2_unit)\n",
    "        self.fc3 = nn.Linear(fc2_unit,action_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # x = state\n",
    "        \"\"\"\n",
    "        Build a network that maps state -> action values.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "import random \n",
    "from collections import namedtuple, deque \n",
    "\n",
    "##Importing the model (function approximator for Q-table)\n",
    "# from model import QNetwork\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler \n",
    "\n",
    "BUFFER_SIZE = int(5*1e5)  #replay buffer size\n",
    "BATCH_SIZE = 128      # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3             # for soft update of target parameters\n",
    "LR = 1e-4            # learning rate\n",
    "UPDATE_EVERY = 4      # how often to update the network\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns form environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        =======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        \n",
    "        #Q- Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(),lr=LR)\n",
    "        \n",
    "        # Replay memory \n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE,BATCH_SIZE,seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_step, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_step, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step+1)% UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get radom subset and learn\n",
    "\n",
    "            if len(self.memory)>BATCH_SIZE:\n",
    "                experience = self.memory.sample()\n",
    "                self.learn(experience, GAMMA)\n",
    "        \n",
    "    def act(self, state, eps = 0):\n",
    "        \"\"\"Returns action for given state as per current policy\n",
    "        Params\n",
    "        =======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "\n",
    "        #Epsilon -greedy action selction\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "            \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        =======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        ## TODO: compute and minimize the loss\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        # Local model is one which we need to train so it's in training mode\n",
    "        self.qnetwork_local.train()\n",
    "        # Target model is one with which we need to get our target so it's in evaluation mode\n",
    "        # So that when we do a forward pass with target model it does not calculate gradient.\n",
    "        # We will update target model weights with soft_update function\n",
    "        self.qnetwork_target.eval()\n",
    "        #shape of output from the model (batch_size,action_dim) = (64,4)\n",
    "        \n",
    "        predicted_targets = self.qnetwork_local(states).gather(1,actions)\n",
    "        with torch.no_grad():\n",
    "            labels_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        # .detach() ->  Returns a new Tensor, detached from the current graph.\n",
    "        labels = rewards + (gamma* labels_next*(1-dones))\n",
    "        loss = criterion(predicted_targets,labels).to(device)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_local,self.qnetwork_target,TAU)\n",
    "            \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        =======\n",
    "            local model (PyTorch model): weights will be copied from\n",
    "            target model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(),\n",
    "                                           local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)\n",
    "            \n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed -size buffe to store experience tuples.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experiences = namedtuple(\"Experience\", field_names=[\"state\",\n",
    "                                                               \"action\",\n",
    "                                                               \"reward\",\n",
    "                                                               \"next_state\",\n",
    "                                                               \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self,state, action, reward, next_state,done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experiences(state,action,reward,next_state,done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory\"\"\"\n",
    "    \n",
    "        experiences = random.sample(self.memory,k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f95e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=7,action_size=21,seed=0)\n",
    "\n",
    "def dqn(env, n_episodes= 1000, max_t = 10000, eps_start=1.0, eps_end = 0.01,\n",
    "       eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training epsiodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon \n",
    "        eps_decay (float): mutiplicative factor (per episode) for decreasing epsilon\n",
    "        \n",
    "    \"\"\"\n",
    "    scores = [] # list containing score from each episode\n",
    "    eps = eps_start\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state,eps)\n",
    "            next_state,reward,done = env.step(action)\n",
    "            agent.step(state,action,reward,next_state,done)\n",
    "            ## above step decides whether we will train(learn) the network\n",
    "            ## actor (local_qnetwork) or we will fill the replay buffer\n",
    "            ## if len replay buffer is equal to the batch size then we will\n",
    "            ## train the network or otherwise we will add experience tuple in our \n",
    "            ## replay buffer.\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                print('episode'+str(i_episode)+':', score)\n",
    "                scores.append(score)\n",
    "                break\n",
    "        eps = max(eps*eps_decay,eps_end)## decrease the epsilon\n",
    "    return scores\n",
    "\n",
    "env = InvOptEnv(demand_hist)\n",
    "scores= dqn(env)\n",
    "\n",
    "plt.plot(np.arange(len(scores)),scores)\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Epsiode #')\n",
    "plt.show()\n",
    "\n",
    "torch.save(agent.qnetwork_local.state_dict(), desired_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e2d4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profit_calculation_sS(s,S,demand_records):\n",
    "    total_profit = 0\n",
    "    inv_level = 25 # inventory on hand, use this to calculate inventory costs\n",
    "    lead_time = 2\n",
    "    capacity = 50\n",
    "    holding_cost = 3\n",
    "    fixed_order_cost = 50\n",
    "    variable_order_cost = 10\n",
    "    unit_price = 30\n",
    "    order_arrival_list = []\n",
    "    for current_period in range(len(demand_records)):\n",
    "        inv_pos = inv_level\n",
    "        if len(order_arrival_list) > 0:\n",
    "            for i in range(len(order_arrival_list)):\n",
    "                inv_pos += order_arrival_list[i][1]\n",
    "        if inv_pos <= s:\n",
    "            order_quantity = min(20,S-inv_pos)\n",
    "            order_arrival_list.append([current_period+lead_time, order_quantity])\n",
    "            y = 1\n",
    "        else:\n",
    "            order_quantity = 0\n",
    "            y = 0\n",
    "        if len(order_arrival_list) > 0:\n",
    "            if current_period == order_arrival_list[0][0]:\n",
    "                inv_level = min(capacity, inv_level + order_arrival_list[0][1])\n",
    "                order_arrival_list.pop(0)\n",
    "        demand = demand_records[current_period]\n",
    "        units_sold = demand if demand <= inv_level else inv_level\n",
    "        profit = units_sold*unit_price-holding_cost*inv_level-y*fixed_order_cost-order_quantity*variable_order_cost\n",
    "        inv_level = max(0,inv_level-demand)\n",
    "        total_profit += profit\n",
    "    return total_profit\n",
    "    \n",
    "s_S_list = []\n",
    "for S in range(1,61): # give a little room to allow S to exceed the capacity \n",
    "    for s in range(0,S):\n",
    "        s_S_list.append([s,S])  \n",
    "        \n",
    "profit_sS_list = []\n",
    "for sS in s_S_list:\n",
    "    profit_sS_list.append(profit_calculation_sS(sS[0],sS[1],demand_hist))\n",
    "\n",
    "best_sS_profit = np.max(profit_sS_list) \n",
    "best_sS = s_S_list[np.argmax(profit_sS_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42674841",
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_test = []\n",
    "for k in range(100,200):\n",
    "    np.random.seed(k)\n",
    "    demand_future = []\n",
    "    for i in range(52):\n",
    "        for j in range(4):\n",
    "            random_demand = np.random.normal(3, 1.5)\n",
    "            if random_demand < 0:\n",
    "                random_demand = 0\n",
    "            random_demand = np.round(random_demand)\n",
    "            demand_future.append(random_demand)\n",
    "        random_demand = np.random.normal(6, 1)\n",
    "        if random_demand < 0:\n",
    "            random_demand = 0\n",
    "        random_demand = np.round(random_demand)\n",
    "        demand_future.append(random_demand)\n",
    "        for j in range(2):\n",
    "            random_demand = np.random.normal(12, 2)\n",
    "            if random_demand < 0:\n",
    "                random_demand = 0\n",
    "            random_demand = np.round(random_demand)\n",
    "            demand_future.append(random_demand)\n",
    "    demand_test.append(demand_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac5a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QNetwork(state_size=7,action_size=21,seed=0)\n",
    "model.load_state_dict(torch.load(desired_path))\n",
    "model.eval()\n",
    "\n",
    "profit_RL = []\n",
    "actions_list = []\n",
    "invs_list = []\n",
    "\n",
    "for demand in demand_test:\n",
    "    env = InvOptEnv(demand)\n",
    "    env.reset()\n",
    "    profit = 0\n",
    "    actions = []\n",
    "    invs = []\n",
    "    done = False\n",
    "    state = env.state\n",
    "    while not done:\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            action_values = model(state)\n",
    "        action = np.argmax(action_values.cpu().data.numpy())\n",
    "        actions.append(action)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        state = next_state\n",
    "        invs.append(env.inv_level)\n",
    "        profit += reward\n",
    "    actions_list.append(actions)\n",
    "    invs_list.append(invs)\n",
    "    profit_RL.append(profit)\n",
    "RL_mean = np.mean(profit_RL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit_sS = []\n",
    "for demand in demand_test:\n",
    "    profit_sS.append(profit_calculation_sS(15,32,demand))\n",
    "sS_mean = np.mean(profit_sS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
